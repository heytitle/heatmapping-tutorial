{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils import logging as lg\n",
    "\n",
    "lg.set_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-08 00:58:32,172 | INFO : plot.py(setup 13) - Setup plot parameters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "from utils import data_provider\n",
    "from notebook_utils import plot\n",
    "plot.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-08 00:58:32,806 | INFO : data_provider.py(get_mnist 17) - Load MNIST : train\n",
      "2017-10-08 00:58:33,141 | INFO : data_provider.py(get_mnist 17) - Load MNIST : test\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = data_provider.get_mnist('train', dir_path='../data/mnist')\n",
    "X_test, Y_test = data_provider.get_mnist('test', dir_path='../data/mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2d = np.copy(X_train.reshape((-1,28,28)))\n",
    "X_test_2d = np.copy(X_test.reshape((-1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-07 21:09:03,787 | INFO : plot.py(show_and_save 20) - save fig to ../figures/nb_figures/mnist-label5-with-cols.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAADJCAYAAAA5I4+DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1J5w42tAYJ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116d792b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_mnist_with_columns(data_idx, col_indices):\n",
    "    plt.figure(figsize=(16,3))\n",
    "\n",
    "    plt.subplot(1,5,1)\n",
    "\n",
    "    plt.imshow(X_train[data_idx,:].reshape((28,28)))\n",
    "    plt.title('Data Label %d' % int(np.sum(np.arange(10) * Y_train[data_idx,:])))\n",
    "\n",
    "    count = 2\n",
    "    for col_idx in col_indices:\n",
    "        plt.subplot(1,5,count)\n",
    "        img_zero = np.zeros((28,28))\n",
    "        img_zero[:, col_idx] = X_train_2d[data_idx, :, col_idx]\n",
    "        plt.title('Col %d-th' % col_idx)\n",
    "        plt.imshow(img_zero, vmin=0, vmax=255)\n",
    "        count = count + 1\n",
    "plot_mnist_with_columns(data_idx = 0, col_indices = [5, 10,15,20])\n",
    "plt.show_and_save(title='mnist-label5-with-cols.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(x, y, n=1):\n",
    "    l = len(x)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield (x[ndx:min(ndx + n, l)],  y[ndx:min(ndx + n, l)])\n",
    "        \n",
    "def build_layer(dims,name):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(dims, stddev=0.1),\n",
    "        name=\"%s_weights\"%name\n",
    "    )\n",
    "\n",
    "    bias = tf.Variable(\n",
    "        tf.zeros(dims[1]),\n",
    "        name=\"%s_bias\"%name\n",
    "    )\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "![image.png](https://i.imgur.com/icDCONr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Notations\n",
    "- $\\boldsymbol{x}^{(\\alpha)}_{(\\cdot,t)}$  : sample $\\alpha$ with all rows of column $t \\in \\{1,\\dots, 28\\}$\n",
    "- $\\boldsymbol{r}^{(\\alpha)}_{t}$ : recurrent inputs of column $t$ of sample $\\alpha$\n",
    "- $\\widetilde{\\boldsymbol{x}}^{(\\alpha)}_{(\\cdot,t)}$ : concatenation of $\\boldsymbol{x}^{(\\alpha)}_{(\\cdot,t)}$  and $\\boldsymbol{r}_{t}$\n",
    "- $\\boldsymbol{W}_{h}$, $\\boldsymbol{b}_{h}$ : weights and bias for hidden unit activations $\\boldsymbol{H}^{(\\alpha)}_t$\n",
    "$$\n",
    "\\boldsymbol{H}^{(\\alpha)}_t = \\text{RELU} \\bigg( \\widetilde{\\boldsymbol{x}}^{(\\alpha)}_{(\\cdot,t)} \\boldsymbol{W}_{h} + \\boldsymbol{b}_{h}  \\bigg)\n",
    "$$\n",
    "- $\\boldsymbol{W}_{o}$, $\\boldsymbol{b}_{o}$ : weights and bias for output unit activations $\\boldsymbol{O}^{(\\alpha)}_t$\n",
    "$$\n",
    "\\boldsymbol{O}^{(\\alpha)}_t = \\text{RELU} \\bigg( \\boldsymbol{H}^{(\\alpha)}_t \\boldsymbol{W}_{o} + \\boldsymbol{b}_{o}  \\bigg)\n",
    "$$\n",
    "- $\\boldsymbol{W}_{r}$, $\\boldsymbol{b}_{r}$ : weights and bias for recurrent unit activations $\\boldsymbol{r}^{(\\alpha)}_{t+1}$\n",
    "$$\n",
    "\\boldsymbol{r}^{(\\alpha)}_{t+1} = \\text{RELU} \\bigg( \\boldsymbol{H}^{(\\alpha)}_t \\boldsymbol{W}_{r} + \\boldsymbol{b}_{r}  \\bigg)\n",
    "$$\n",
    "- $\\hat{\\boldsymbol{y}}^{(\\alpha)}$ : class predictions of $\\boldsymbol{x}^{(\\alpha)}$\n",
    "$$\n",
    "\\hat{\\boldsymbol{y}}^{(\\alpha)} = \\text{SOFTMAX} \\bigg( \\boldsymbol{O}^{(\\alpha)}_{28}  \\bigg) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters are choosed arbitrarily.\n",
    "NETWORK_PARAMS = {\n",
    "    'batch_size': 50,\n",
    "    'pior_cell_layer_units': 100,\n",
    "#     'pior_output_units': 300,\n",
    "    'hidden_units': 60,\n",
    "    'output_units': 10,\n",
    "    'recurrent_units': 7,\n",
    "    'input_max_seq_length': 28,\n",
    "    'input_dims': 28,\n",
    "    'learning_rate': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-08 22:35:06,213 | INFO : <ipython-input-172-fff692566896>(train_and_evaluate 3) - Training 4 columns at a time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 112)\n",
      "(?, 112)\n",
      "(?, 112)\n",
      "(?, 112)\n",
      "(?, 112)\n",
      "(?, 112)\n",
      "(?, 112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-08 22:35:30,046 | INFO : <ipython-input-172-fff692566896>(train_and_evaluate 48) - epoch 1\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(seq_length=1, epoch=2, _lr=NETWORK_PARAMS['learning_rate'], debug=False):\n",
    "    no_input_cols = NETWORK_PARAMS['input_max_seq_length'] // seq_length\n",
    "    logging.info('Training %d columns at a time' % no_input_cols)\n",
    "\n",
    "    recurrent_inputs = tf.placeholder(tf.float32, shape=(None, NETWORK_PARAMS['recurrent_units']))\n",
    "\n",
    "\n",
    "    input_weights, input_bias = build_layer((NETWORK_PARAMS['input_dims']*no_input_cols, NETWORK_PARAMS['pior_cell_layer_units']), 'input')\n",
    "    input_tc_weights, input_tc_bias = build_layer((NETWORK_PARAMS['pior_cell_layer_units'] +NETWORK_PARAMS['recurrent_units'] , NETWORK_PARAMS['hidden_units']), 'input_tc')\n",
    "    \n",
    "    output_weights, output_bias = build_layer((NETWORK_PARAMS['hidden_units'], NETWORK_PARAMS['output_units']), 'output')\n",
    "    \n",
    "    recurrent_output_weights, recurrent_output_bias = build_layer((NETWORK_PARAMS['hidden_units'], NETWORK_PARAMS['recurrent_units']), 'recurrent')\n",
    "\n",
    "\n",
    "    x_input = tf.placeholder(tf.float32, shape=(None, NETWORK_PARAMS['input_dims'], NETWORK_PARAMS['input_dims']))\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    rr = recurrent_inputs\n",
    "    \n",
    "\n",
    "    for i in range(0, x_input.shape[1], no_input_cols):\n",
    "        ii = tf.reshape(x_input[:,i:i+no_input_cols], [-1, no_input_cols * NETWORK_PARAMS['input_dims'] ])\n",
    "        print(ii.shape)\n",
    "        itc = tf.nn.relu(tf.matmul(ii, input_weights) + input_bias)\n",
    "        \n",
    "        xr = tf.concat([itc, rr], axis=1)\n",
    "        ha = tf.nn.relu(tf.matmul(xr, input_tc_weights) + input_tc_bias)\n",
    "        ot = tf.matmul(ha, output_weights) + output_bias\n",
    "        rr = tf.nn.relu(tf.matmul(ha, recurrent_output_weights) + recurrent_output_bias)\n",
    "\n",
    "    y = ot\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=_lr).minimize(loss_op)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        step = 1\n",
    "        for i in range(epoch):\n",
    "            logging.info('epoch %d' % (i+1))\n",
    "            for xf, yf in batch_data(X_train_2d, Y_train, n=NETWORK_PARAMS['batch_size']):\n",
    "        \n",
    "                rr = np.zeros((NETWORK_PARAMS['batch_size'], NETWORK_PARAMS['recurrent_units']))\n",
    "                sess.run(train_op, feed_dict={x_input: xf, y_: yf, recurrent_inputs: rr})\n",
    "                \n",
    "                if (step % 500 == 0 or step < 10) and debug:\n",
    "                    rr = np.zeros((len(yf), NETWORK_PARAMS['recurrent_units']))\n",
    "                    acc, loss = sess.run([accuracy, loss_op], feed_dict={x_input: xf, y_: yf, recurrent_inputs:rr})\n",
    "                    logging.info('step %d : train acc %f, loss %f' % (step, acc, loss))\n",
    "                    \n",
    "                step = step + 1\n",
    "                \n",
    "        rr = np.zeros((len(mnist.test.labels), NETWORK_PARAMS['recurrent_units']))\n",
    "        acc = sess.run(accuracy, feed_dict={x_input: X_test_2d, y_: Y_test, recurrent_inputs: rr})\n",
    "        print('Seq Length %d (%d epoch trained): Test acc %f' % (seq_length, epoch, acc))\n",
    "        return acc\n",
    "    \n",
    "train_and_evaluate(seq_length=7, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-08 13:23:35,668 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 3) - Training 4 columns at a time\n",
      "2017-10-08 13:23:53,470 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 1\n",
      "2017-10-08 13:23:54,028 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 1 : train acc 0.360000, loss 2.263072\n",
      "2017-10-08 13:23:54,035 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 2 : train acc 0.300000, loss 2.230270\n",
      "2017-10-08 13:23:54,042 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 3 : train acc 0.300000, loss 2.240301\n",
      "2017-10-08 13:23:54,048 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 4 : train acc 0.200000, loss 2.182860\n",
      "2017-10-08 13:23:54,055 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 5 : train acc 0.320000, loss 2.166144\n",
      "2017-10-08 13:23:54,062 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 6 : train acc 0.240000, loss 2.166861\n",
      "2017-10-08 13:23:54,070 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 7 : train acc 0.240000, loss 2.024304\n",
      "2017-10-08 13:23:54,077 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 8 : train acc 0.200000, loss 1.951886\n",
      "2017-10-08 13:23:54,083 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 9 : train acc 0.220000, loss 1.967808\n",
      "2017-10-08 13:23:55,426 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 500 : train acc 0.840000, loss 0.496318\n",
      "2017-10-08 13:23:56,791 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 1000 : train acc 0.760000, loss 0.623531\n",
      "2017-10-08 13:23:57,335 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 2\n",
      "2017-10-08 13:23:58,144 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 1500 : train acc 0.840000, loss 0.563578\n",
      "2017-10-08 13:23:59,489 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 2000 : train acc 0.840000, loss 0.476096\n",
      "2017-10-08 13:24:00,569 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 3\n",
      "2017-10-08 13:24:00,846 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 2500 : train acc 0.820000, loss 0.603220\n",
      "2017-10-08 13:24:02,205 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 3000 : train acc 0.880000, loss 0.450753\n",
      "2017-10-08 13:24:03,550 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 3500 : train acc 0.860000, loss 0.560662\n",
      "2017-10-08 13:24:03,826 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 4\n",
      "2017-10-08 13:24:04,902 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 4000 : train acc 0.880000, loss 0.332961\n",
      "2017-10-08 13:24:06,240 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 4500 : train acc 0.900000, loss 0.283506\n",
      "2017-10-08 13:24:07,050 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 5\n",
      "2017-10-08 13:24:07,594 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 5000 : train acc 0.960000, loss 0.139114\n",
      "2017-10-08 13:24:08,939 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 5500 : train acc 0.980000, loss 0.128861\n",
      "2017-10-08 13:24:10,282 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 6000 : train acc 0.900000, loss 0.253860\n",
      "2017-10-08 13:24:10,283 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 6\n",
      "2017-10-08 13:24:11,633 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 6500 : train acc 0.960000, loss 0.283116\n",
      "2017-10-08 13:24:12,988 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 7000 : train acc 0.860000, loss 0.448533\n",
      "2017-10-08 13:24:13,537 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 7\n",
      "2017-10-08 13:24:14,344 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 7500 : train acc 0.920000, loss 0.323928\n",
      "2017-10-08 13:24:15,692 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 8000 : train acc 0.960000, loss 0.124202\n",
      "2017-10-08 13:24:16,772 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 8\n",
      "2017-10-08 13:24:17,049 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 8500 : train acc 0.920000, loss 0.392346\n",
      "2017-10-08 13:24:18,390 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 9000 : train acc 0.920000, loss 0.283184\n",
      "2017-10-08 13:24:19,733 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 9500 : train acc 0.900000, loss 0.258895\n",
      "2017-10-08 13:24:20,011 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 9\n",
      "2017-10-08 13:24:21,100 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 10000 : train acc 0.900000, loss 0.396208\n",
      "2017-10-08 13:24:22,468 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 10500 : train acc 0.940000, loss 0.206135\n",
      "2017-10-08 13:24:23,278 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 47) - epoch 10\n",
      "2017-10-08 13:24:23,819 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 11000 : train acc 0.980000, loss 0.111895\n",
      "2017-10-08 13:24:25,162 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 11500 : train acc 0.900000, loss 0.188370\n",
      "2017-10-08 13:24:26,511 | INFO : <ipython-input-168-b606a74ff083>(train_and_evaluate 56) - step 12000 : train acc 0.980000, loss 0.095522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq Length 7 (10 epoch trained): Test acc 0.901500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90149999"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(seq_length=7, epoch=10, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_runs = 10\n",
    "# accs = []\n",
    "# for i in range(no_runs):\n",
    "#     logging.info('Run %d' % (i+1))\n",
    "#     acc = train_and_evaluate()\n",
    "#     accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy over 10 runs : 0.10±0.01\n",
      "max acc: 0.11\n",
      "min acc: 0.09\n"
     ]
    }
   ],
   "source": [
    "# print('Average accuracy over %d runs : %.2f±%.2f' % (no_runs, np.mean(accs), np.std(accs)))\n",
    "# print('max acc: %.2f' % np.max(accs))\n",
    "# print('min acc: %.2f' % np.min(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed k columns at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def train_and_evaluate_network2(_lr=0.01, seq_lenghinput_dims=28, recurrent_units=7, output_units=10, hidden_units=100):\n",
    "#     recurrent_inputs = tf.placeholder(tf.float32, shape=(None, NETWORK_PARAMS['recurrent_units']))\n",
    "\n",
    "#     input_weights = tf.Variable(\n",
    "#         tf.truncated_normal(\n",
    "#             (\n",
    "#              NETWORK_PARAMS['input_dims'] + NETWORK_PARAMS['recurrent_units'],\n",
    "#              NETWORK_PARAMS['hidden_units']\n",
    "#             ),\n",
    "#             stddev=0.1\n",
    "#         ),\n",
    "#         name=\"input_weights\"\n",
    "#     )\n",
    "\n",
    "#     input_bias = tf.Variable(\n",
    "#         tf.zeros(\n",
    "#             (\n",
    "#              NETWORK_PARAMS['hidden_units']\n",
    "#             ),\n",
    "#         ),\n",
    "#         name=\"input_bias\"\n",
    "#     )\n",
    "\n",
    "#     output_weights = tf.Variable(\n",
    "#         tf.truncated_normal(\n",
    "#             (NETWORK_PARAMS['hidden_units'], NETWORK_PARAMS['output_units']),\n",
    "#             stddev=0.1\n",
    "#         ),\n",
    "#         name=\"output_weights\"\n",
    "#     )\n",
    "\n",
    "#     output_bias = tf.Variable(\n",
    "#         tf.zeros(\n",
    "#             (\n",
    "#              NETWORK_PARAMS['output_units']\n",
    "#             ),\n",
    "#         ),\n",
    "#         name=\"output_bias\"\n",
    "#     )\n",
    "\n",
    "#     recurrent_output_weights = tf.Variable(\n",
    "#         tf.truncated_normal(\n",
    "#             (NETWORK_PARAMS['hidden_units'], NETWORK_PARAMS['recurrent_units']),\n",
    "#             stddev=0.1\n",
    "#         ),\n",
    "#         name=\"recurrent_output_weights\"\n",
    "#     )\n",
    "\n",
    "#     recurrent_output_bias = tf.Variable(\n",
    "#         tf.zeros(\n",
    "#             (\n",
    "#              NETWORK_PARAMS['recurrent_units']\n",
    "#             ),\n",
    "#         ),\n",
    "#         name=\"recurrent_output_bias\"\n",
    "#     )\n",
    "\n",
    "#     x_input = tf.placeholder(tf.float32, shape=(None, NETWORK_PARAMS['input_dims'], NETWORK_PARAMS['input_dims']))\n",
    "#     y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#     rr = recurrent_inputs\n",
    "#     for i in range(NETWORK_PARAMS['input_seq_length']):\n",
    "#         xr = tf.concat([ x_input[:,i], rr], axis=1)\n",
    "#         ha = tf.nn.relu(tf.matmul(xr, input_weights) + input_bias)\n",
    "#         ot = tf.nn.relu(tf.matmul(ha, output_weights) + output_bias)\n",
    "#         rr = tf.nn.relu(tf.matmul(ha, recurrent_output_weights) + recurrent_output_bias)\n",
    "\n",
    "#     y = tf.nn.softmax(ot)\n",
    "\n",
    "#     cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate=_lr).minimize(cross_entropy)\n",
    "\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(init_op)\n",
    "\n",
    "#         for i in range(NETWORK_PARAMS['train_epochs']):\n",
    "#             logging.info('epoch %d' % i)\n",
    "#             for xf, yf in batch_data(X_train_2d, Y_train, n=NETWORK_PARAMS['batch_size']):\n",
    "#                 rr = np.zeros((NETWORK_PARAMS['batch_size'], NETWORK_PARAMS['recurrent_units']))\n",
    "#                 train_step.run(feed_dict={x_input: xf, y_: yf, recurrent_inputs: rr})\n",
    "\n",
    "#         correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#         rr = np.zeros((len(Y_test), NETWORK_PARAMS['recurrent_units']))\n",
    "\n",
    "#         acc = accuracy.eval(feed_dict={x_input: X_test_2d, y_: Y_test, recurrent_inputs: rr})\n",
    "#         logging.info('Accuracy %.2f%%' % (acc*100))\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "4 8\n",
      "8 12\n",
      "12 16\n",
      "16 20\n",
      "20 24\n",
      "24 28\n"
     ]
    }
   ],
   "source": [
    "total = 28\n",
    "seq_length = 7\n",
    "no_col = total // seq_length\n",
    "\n",
    "for i in range(0, total, no_col):\n",
    "    print(i, i + no_col)\n",
    "# print(no_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layer(dims,name):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal(dims, stddev=0.1),\n",
    "        name=\"%s_weights\"%name\n",
    "    )\n",
    "\n",
    "    bias = tf.Variable(\n",
    "        tf.zeros(dims[1]),\n",
    "        name=\"%s_bias\"%name\n",
    "    )\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 208.603729, training acc 0.109375)\n",
      "Loss 91.194565, training acc 0.343750)\n",
      "Loss 1.995476, training acc 0.226562)\n",
      "Loss 1.843038, training acc 0.312500)\n",
      "Loss 1.825322, training acc 0.257812)\n",
      "Loss 1.618306, training acc 0.375000)\n",
      "Test acc 0.340300\n"
     ]
    }
   ],
   "source": [
    "def simple_relu_net():\n",
    "    learning_rate=0.1\n",
    "    x_input = tf.placeholder(tf.float32, shape=(None, NETWORK_PARAMS['input_dims']*NETWORK_PARAMS['input_dims']))\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    l1_w, l1_b = build_layer((784,256), 'l1')\n",
    "    lx_w, lx_b = build_layer((256,256), 'lx')\n",
    "    l2_w, l2_b = build_layer((256,10), 'l2')\n",
    "    \n",
    "    mt = tf.nn.relu(tf.matmul(x_input, l1_w) + l1_b )\n",
    "    gg = tf.nn.relu(tf.matmul(mt, lx_w) + lx_b )\n",
    "    ot = tf.matmul(gg, l2_w) + l2_b\n",
    "     \n",
    "    y = ot\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        for i in range(500):\n",
    "            batch = mnist.train.next_batch(128)\n",
    "            \n",
    "            sess.run(train_op, feed_dict={x_input: batch[0], y_: batch[1]})\n",
    "            \n",
    "            if i % 100 == 0 or i == 1:\n",
    "                \n",
    "                acc, loss = sess.run([accuracy, loss_op], feed_dict={x_input: batch[0], y_: batch[1]})\n",
    "                print('Loss %f, training acc %f)' %(loss, acc))\n",
    "#                 print(\"Train Accuracy : %.4f\" % sess.run(accuracy, feed_dict={x_input: X_train_2d.reshape(-1,28*28), y_: Y_train}))\n",
    "#                 print(\"Train Accuracy : %.4f\" % sess.run(accuracy, feed_dict={x_input: mnist.train.images, y_: mnist.train.labels}))\n",
    "        print('Test acc %f' % sess.run(accuracy, feed_dict={x_input: mnist.test.images, y_: mnist.test.labels}))\n",
    "simple_relu_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug!\n",
    "def batch_data(x, y, n=1):\n",
    "    l = len(x)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield (x[ndx:min(ndx + n, l)],  y[ndx:min(ndx + n, l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "b = [-1,-2,-3,-4,-5,-6]\n",
    "\n",
    "for i,j in batch_data(X_train_2d, Y_train, n=100):\n",
    "    print(i[0,:])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.41176474,  0.89019614,  0.99215692,  0.99215692,  0.23529413,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.45490199,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.98823535,  0.57647061,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00784314,  0.63529414,  0.97647065,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.98823535,  0.36862746,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.63529414,  0.98823535,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.54509807,  0.05490196,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.45490199,  0.97647065,  0.98823535,  0.98823535,\n",
       "         0.92549026,  0.57647061,  0.5411765 ,  0.05490196,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.2392157 ,  0.99215692,  0.99215692,  0.99215692,  0.92549026,\n",
       "         0.17647059,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.3137255 ,\n",
       "         0.89019614,  0.98823535,  0.98823535,  0.90588242,  0.38431376,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.16078432,  0.87450987,\n",
       "         0.99215692,  0.98823535,  0.98431379,  0.50588238,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.66274512,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.52549022,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.02352941,  0.25098041,  0.25098041,\n",
       "         0.21176472,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.03529412,  0.80000007,  0.98823535,\n",
       "         0.99215692,  0.71372551,  0.01568628,  0.        ,  0.        ,\n",
       "         0.41960788,  0.72549021,  0.84313732,  0.98823535,  0.98823535,\n",
       "         0.96862751,  0.51372552,  0.03529412,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.74117649,  0.99215692,  0.99215692,\n",
       "         0.90196085,  0.27843139,  0.15294118,  0.44705886,  0.92549026,\n",
       "         1.        ,  0.99215692,  0.99215692,  0.99215692,  0.99215692,\n",
       "         1.        ,  0.99215692,  0.96078438,  0.19215688,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.90980399,  0.98823535,  0.98823535,\n",
       "         0.6901961 ,  0.1137255 ,  0.93725497,  0.98823535,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.68235296,  0.65882355,  0.7960785 ,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.63921571,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.03137255,  0.91764712,  0.98823535,  0.95686281,\n",
       "         0.29019609,  0.83529419,  0.98823535,  0.98823535,  0.95686281,\n",
       "         0.66666669,  0.18431373,  0.00784314,  0.        ,  0.03529412,\n",
       "         0.23529413,  0.98823535,  0.98823535,  0.74117649,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.33333334,  0.98823535,  0.98823535,  0.57647061,\n",
       "         0.27058825,  0.98823535,  0.98823535,  0.82352948,  0.25882354,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.27058825,  0.98823535,  0.98823535,  0.74117649,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.50196081,  0.98823535,  0.98823535,  0.57647061,\n",
       "         0.40784317,  0.98823535,  0.98823535,  0.60392159,  0.24313727,\n",
       "         0.41960788,  0.41568631,  0.03529412,  0.        ,  0.34901962,\n",
       "         0.92549026,  0.98823535,  0.98823535,  0.39607847,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.74901962,  0.99215692,  0.99215692,  0.57647061,\n",
       "         0.14117648,  0.88235301,  0.99215692,  0.99215692,  0.99215692,\n",
       "         0.83137262,  0.47058827,  0.23529413,  0.85490203,  0.99215692,\n",
       "         1.        ,  0.99215692,  0.80784321,  0.13725491,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.60784316,  0.98823535,  0.98823535,  0.71372551,\n",
       "         0.14117648,  0.08235294,  0.28235295,  0.38431376,  0.38431376,\n",
       "         0.50196081,  0.82745105,  0.98823535,  0.98823535,  0.98823535,\n",
       "         0.99215692,  0.74117649,  0.05490196,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.16862746,  0.94901967,  0.98823535,  0.98823535,\n",
       "         0.94901967,  0.90980399,  0.90980399,  0.90980399,  0.9450981 ,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.98823535,  0.95686281,\n",
       "         0.42745101,  0.01960784,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.52941179,  0.98823535,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.98823535,  0.98823535,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.92549026,  0.25882354,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.01568628,  0.26666668,  0.57647061,\n",
       "         0.99215692,  0.98823535,  0.98823535,  0.98823535,  0.81568635,\n",
       "         0.57647061,  0.26666668,  0.16470589,  0.12156864,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(100)[0][0,:].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 Thesis Kernel",
   "language": "python",
   "name": "py3-thesis-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "670px",
    "left": "0px",
    "right": "20px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
