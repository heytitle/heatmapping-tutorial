{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability of neural networks\n",
    "\n",
    "\n",
    "## Model Explanation\n",
    "Activiation Maximization : Finding a prototype of each target class.\n",
    "\n",
    "**TODO** : add more details and example\n",
    "\n",
    "## Explaining DNN Decisions\n",
    "**NOTATIONS**\n",
    "\n",
    "$R_i(\\boldsymbol{x})$ = relevance score of pixel $i$ of an image $\\boldsymbol{x}$\n",
    "\n",
    "$f(\\boldsymbol{x})$ = unnormalized prediction score ( aka. activation values before softmax layer )\n",
    "\n",
    "\n",
    "### Sensitivity Analysis\n",
    "Computing rate of change of output respect to input variables.\n",
    "\n",
    "```\n",
    "What makes an image more/less a car?\n",
    "```\n",
    "$$\n",
    "R_i(\\boldsymbol{x}) = \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i}\n",
    "$$\n",
    "Commonly people use :\n",
    "$$\n",
    "R_i(\\boldsymbol{x}) = \\Big ( \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i} \\Big )^2\n",
    "$$\n",
    "DOUBT\n",
    "- Why is it better to use $(..)^2$?\n",
    "- Does Saliency map use this method?\n",
    "\n",
    "### Simple Taylor Decomposion\n",
    "Decomposing $f(\\boldsymbol{x})$ as sum of $R_i(\\boldsymbol{x})$\n",
    "$$\n",
    "f(\\boldsymbol{x}) = \\sum_{i=1}^{d}{ R_i(\\boldsymbol{x}) }  + O(\\boldsymbol{x}\\boldsymbol{x}^T)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "R_i(\\boldsymbol{x}) = \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i} \n",
    "\\Bigg |_{\\boldsymbol{x} = \\boldsymbol{\\widetilde{x}}} \\cdot ( x_i - \\widetilde{x}_i )\n",
    "$$\n",
    "DOUBT\n",
    "- Is $x_i - \\widetilde{x}_i$ a vector or scalar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation of Simple Taylor Decomposion\n",
    "\\begin{align}\n",
    "f(\\boldsymbol{x})  &= f(\\boldsymbol{\\widetilde{x}}) + \\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x}) \n",
    "\\big |_{\\boldsymbol{x} = \\boldsymbol{\\widetilde{x}}} \\cdot ( \\boldsymbol{x} - \\boldsymbol{ \\widetilde{x} } ) + O(\\nabla_{\\boldsymbol{x}}^{2} ) + O(\\nabla_{\\boldsymbol{x}}^{3} ) + ...\n",
    "\\\\\n",
    "&= f(\\boldsymbol{\\widetilde{x}}) + \\sum_{i=1}^{d}{\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i}} \\Big |_{x_i = \\widetilde{x}_i}  ( x_i - \\widetilde{x}_i ) + O(\\boldsymbol{x}\\boldsymbol{x}^T) \\\\\n",
    "&= \\sum_{i=1}^{d}{\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i}} \\Big |_{x_i = \\widetilde{x}_i}  ( x_i - \\widetilde{x}_i ) +\n",
    "O(\\boldsymbol{x}\\boldsymbol{x}^T)\n",
    "\\tag*{(choose  $\\boldsymbol{\\widetilde{x}}  \\to f(\\boldsymbol{\\widetilde{x}}) = 0 $  )}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Network without biases\n",
    "For networks that based on **ReLU** activator: $\\textrm{RL}(x) = max(0, x)$,  we can omit $O(\\boldsymbol{x}\\boldsymbol{x}^T)$ because $$\\forall i \\ge 2 : \\nabla_{\\boldsymbol{x}}^{i}f(\\boldsymbol{x}) = 0 $$\n",
    "and $\\boldsymbol{\\widetilde{x}}$ can be found by :\n",
    "$$\n",
    "\\boldsymbol{\\widetilde{x}} = \\lim_{\\epsilon \\to 0} \\epsilon \\boldsymbol{x} \\tag*{DOUBT : what does this mean?}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lim_{\\epsilon \\to 0} f(\\epsilon \\boldsymbol{x}) = \\lim_{\\epsilon \\to 0} \\epsilon f(\\boldsymbol{x})\n",
    "$$\n",
    "\n",
    "As a result, \n",
    "$$\n",
    "f(\\boldsymbol{x}) = \\sum_{i=1}^{d} R_i({\\boldsymbol{x}})\n",
    "$$\n",
    "and the relevance score is :\n",
    "$$\n",
    "R_i(\\boldsymbol{x}) = \\frac{\\partial f(\\boldsymbol{x}) }{\\partial x_i } x_i \\tag*{DOUBT : how?}\n",
    "$$ \n",
    "where $ \\frac{\\partial f(\\boldsymbol{x}) }{\\partial x_i } $ is sensivity and $x_i$ is saliency of input value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deriavation of $R_i(\\boldsymbol{x})$\n",
    "\n",
    "\\begin{align}\n",
    "f'(\\widetilde{x}) &= \\lim_{ h \\to 0} \\frac{ f( \\widetilde{x}+ h ) - f(\\widetilde{x})}{h} \\\\ \n",
    "&= \\lim_{ h \\to 0} \\frac{ f( cx+ h ) - f(cx)}{h} \\tag*{($\\widetilde{x} = \\lim_{\\epsilon \\to 0} \\epsilon x = cx$)} \\\\\n",
    "&= \\lim_{ h \\to 0} \\frac{ cf(x+ h/c) - cf(x)}{h} \\tag*{(ReLU property)} \\\\\n",
    "&= c\\lim_{ h \\to 0} \\frac{ f(x+ h/c) - f(x)}{h} \\\\\n",
    "&= c\\lim_{ h \\to 0} \\frac{ f(x+ h/c) - f(x)}{h} \\cdot \\frac{c}{c} \\\\\n",
    "&= c\\lim_{ h \\to 0} \\frac{ f(x+ h/c) - f(x)}{h/c} \\cdot \\frac{1}{c} \\\\\n",
    "&= f'(x)\n",
    "\\end{align}\n",
    "Thus,\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i} \n",
    "\\Bigg |_{\\boldsymbol{x} = \\boldsymbol{\\widetilde{x}}}  = \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance Propagation\n",
    "- Exploit feed-forward graph of the model. Redistributing prediction score back until input pixels.\n",
    "- Redistribution rule must satisfy **relevance conservation** priniciple ( aka no enery loss during the process )\n",
    "\n",
    "**NOTATION**\n",
    "- $R_{j \\leftarrow k }$ is relevance score propagated from neuron $k$ to $j$ where $k$ in layer **i-th** and $j$ is in **(i-1)-th**\n",
    "\n",
    "To satisfy  **relevance conservation**, we need :\n",
    "\n",
    "- $\\sum_{j} R_{j \\leftarrow k} = R_k$\n",
    "- $R_j = \\sum_{k} R_{j \\leftarrow k}$\n",
    "\n",
    "Putting these constraints together we have :\n",
    "\\begin{align}\n",
    "f(x) &= \\sum_{k} R_{k} \n",
    "= \\sum_{k} \\sum_{j} R_{j \\leftarrow k} \n",
    "= \\sum_{j} \\sum_{k} R_{j \\leftarrow k} \n",
    "= \\sum_{j} R_{j} \n",
    "= \\cdots  = \\sum_{i=1}^{d} R_i\n",
    "\\end{align}\n",
    "\n",
    "Advantages of decomposition method:\n",
    "- DOUBT : if the number of input variables  is limited, the analysis can be represented as a pie chart, histogram\n",
    "- Pooling. For example, relevance scores of RBG channels can be aggregated and represented as the score of that pixel.\n",
    "- Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer-wise Relevance Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods that exploit network structure but don't use decomposition:\n",
    "- TODO: deconvolution \n",
    "- TODO: guided back-prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "630px",
    "left": "0px",
    "right": "1163px",
    "top": "106px",
    "width": "273px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
